{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a732c-7119-4a58-9f07-e4ace75a7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae3bf7",
   "metadata": {},
   "source": [
    "# Experimental Results\n",
    "\n",
    "Here, we will plot the results of running Q-learning on different maps and specifications, along with different reward strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ea564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from data_utils import load_data, create_data_table, plot_curve\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "plt.style.use(['science', \"ieee\"])\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",   # specify font family here\n",
    "    \"font.serif\": [\"Times\"],  # specify font here\n",
    "})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10e5f8",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will need to set the directory where the logged data is stored. Then, we can use the `load_data` and `create_data_table` functions to extract the data from the stored CSV files into plottable `DataFrame`s.\n",
    "\n",
    "As we can see, for each reward method, the `DataFrame` returned contains a row entry for each evaluation episode. Each row is indexed by the training iteration number of the evaluated policy (`training_iter`) and contains the total reward obtained from that evaluation run (`total_reward`), the number of accepting visits in the product automaton (`acc_visits`), and whether the run is accepting or not (`accepting`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = Path(\"../logs/2022-01-27-080044\")\n",
    "MAP = \"map02\"\n",
    "SPEC = \"bounded_recurrence1\"\n",
    "METHODS = [\"sparse\", \"true-pot\", \"lavaei2020\", \"tauMDP\"]\n",
    "\n",
    "DATA = {\n",
    "    method: load_data(LOGDIR, MAP, SPEC, method)\n",
    "    for method in METHODS\n",
    "}\n",
    "\n",
    "DATA[\"true-pot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbc1bc",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "\n",
    "For each method, we want to plot the probability of acceptance --- which is the average number of accepting runs at a checkpoint --- and the total reward obtained at a checkpoint. This means that we want to plot the average `acceptance` and average `total_reward` against the training iteration `training_iter`.\n",
    "\n",
    "To do this, we will first create a `DataFrame` for each of the necessary quantities we want to aggregate, with the columns as the method used. This can be done using the `create_data_table` method defined in `data_utils`. We will then need to aggregate the data across training iterations, compute the error bands with a **95% confidence interval**, and plot the smoothened curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a7c9b",
   "metadata": {},
   "source": [
    "## Plotting Probability of Acceptance\n",
    "\n",
    "Given a set of runs labelled with a `True` or a `False` based on if the run is accepting or not, the probability of acceptance is modelled by the binomial distribution associated with all the runs (each of which is essentially a Bernoulli trial).\n",
    "\n",
    "Thus, we use the Agrestiâ€“Coull interval formula to calculate the 95% confidence interval for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0745aef-fe20-45ae-a34c-a0a6e347ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, sharex=True)\n",
    "ax1.set_ylabel(\"Probability of Satisfaction\")\n",
    "ax1.set_xlabel(\"No. of Training Iterations\")\n",
    "\n",
    "# Probability of accepting\n",
    "accepting_data = create_data_table(\"accepting\", *DATA.items())\n",
    "for method in accepting_data.columns:\n",
    "    data = accepting_data[method]\n",
    "    plot_curve(method, ax1, data, span=5, confidence=0.95, bernoulli=True)\n",
    "\n",
    "\n",
    "ax1.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b8cf7",
   "metadata": {},
   "source": [
    "### Plotting average total rewards\n",
    "\n",
    "Since each reward has a different scale, it doesn't really make sense to plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd68e7-e2a3-44c2-963c-76889e3b45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, sharex=True)\n",
    "ax1.set_ylabel(\"Average total rewards\")\n",
    "ax1.set_xlabel(\"No. of Training Iterations\")\n",
    "\n",
    "# Probability of accepting\n",
    "total_reward_data = create_data_table(\"total_reward\", *DATA.items())\n",
    "for method in accepting_data.columns:\n",
    "    data = total_reward_data[method]\n",
    "    plot_curve(method, ax1, data, span=5, confidence=0.95, bernoulli=False)\n",
    "\n",
    "\n",
    "ax1.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0411bb5",
   "metadata": {},
   "source": [
    "# $\\tau$-MDP Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b596e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU_DATA_DIR = Path(\"./logs/tau\")\n",
    "TAU_ENV = 2\n",
    "COLS = [\"training_iter\", \"total_reward\", \"robustness\", \"accepting\"]\n",
    "\n",
    "data_files = list(TAU_DATA_DIR.glob(f\"tauMDP_ENV_{TAU_ENV}_pslip_0.1_*.csv\"))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for f in data_files:\n",
    "    df: pd.DataFrame = pd.read_csv(\n",
    "        f, index_col=False, names=COLS, header=0\n",
    "    )  # type: ignore\n",
    "    data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "if len(data) > 0:\n",
    "    data.set_index(\"training_iter\", inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ca180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, sharex=True)\n",
    "ax1.set_ylabel(\"Probability of Satisfaction\")\n",
    "ax1.set_xlabel(\"No. of Training Iterations\")\n",
    "\n",
    "# Probability of accepting\n",
    "accepting_data = create_data_table(\"accepting\", (\"tauMDP\", data))\n",
    "plot_curve(\"tauMDP\", ax1, accepting_data[\"tauMDP\"], span=5, confidence=0.95, bernoulli=True)\n",
    "\n",
    "\n",
    "ax1.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3bcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, sharex=True)\n",
    "ax1.set_ylabel(\"Probability of Satisfaction\")\n",
    "ax1.set_xlabel(\"No. of Training Iterations\")\n",
    "\n",
    "# Probability of accepting\n",
    "accepting_data = create_data_table(\"accepting\", (\"tauMDP\", data))\n",
    "plot_curve(\"tauMDP\", ax1, accepting_data[\"tauMDP\"], span=5, confidence=0.95, bernoulli=True)\n",
    "\n",
    "\n",
    "ax1.legend(loc=\"upper right\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
